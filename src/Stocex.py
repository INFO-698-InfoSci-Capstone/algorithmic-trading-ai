# -*- coding: utf-8 -*-
"""Stocex_nb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1il9tnnU4GzqnlHwKxhO0pj8IY5IqzPEs

## **Stocex project ipynb file.**
"""

# Necessary libraries
#!pip install yfinance
#!pip install newspaper3k
#!pip install transformers torch
#!pip install yfinance
!pip install chronos-ts --upgrade --quiet
import json

"""## ***Step 1*** - Retrieve latest news from NewsAPI

Pull yesterday‚Äôs financial news headlines using NewsAPI for the latest market-moving events
"""

# Code to retrieve yesterday news from NewSAPI.

import requests
import pandas as pd
from datetime import datetime, timedelta

# üîë Enter your NewsAPI key here
NEWSAPI_KEY = "c32779e494d04276b24ac0eb577c5ca2"

def fetch_yesterdays_news():
    yesterday = datetime.now() - timedelta(days=1)
    date_str = yesterday.strftime("%Y-%m-%d")

    query = (
    "stocks OR stock OR market OR earnings OR inflation OR layoffs OR fed OR economic data "
    "OR acquisition OR merger OR buyout OR billion OR million OR IPO OR funding "
    "OR forecast OR guidance OR quarterly results OR revenue OR profits OR shares "
    "OR dividends OR buybacks OR takeover OR analysts OR downgrade OR upgrade"
    )

    domains = (
        "bloomberg.com,cnn.com,cnbc.com,wsj.com,reuters.com,marketwatch.com,"
        "yahoo.com,investopedia.com,seekingalpha.com,fool.com,fortune.com,"
        "forbes.com,techcrunch.com,businessinsider.com,barrons.com"
    )

    url = (
        f"https://newsapi.org/v2/everything?q={query}"
        f"&from={date_str}&to={date_str}"
        f"&language=en&sortBy=publishedAt"
        f"&pageSize=100"
        f"&domains={domains}"
        f"&apiKey={NEWSAPI_KEY}"
    )

    response = requests.get(url)
    data = response.json()

    if "articles" in data:
        articles = data["articles"]
        df = pd.DataFrame([{
            "title": article["title"],
            "description": article["description"],
            "publishedAt": article["publishedAt"],
            "source": article["source"]["name"]
        } for article in articles])
        return df
    else:
        print("No articles found or error in API call.")
        return pd.DataFrame()

# üöÄ Run it
news_df = fetch_yesterdays_news()
news_df.head(10)  # Preview the headlines

"""##***Step 2*** - Extract Tickers from Yesterday‚Äôs News

Identify all the stock tickers mentioned in yesterday‚Äôs headlines using NLP
"""

import spacy
nlp = spacy.load("en_core_web_sm")

#Loading company names and tickers
import pandas as pd

def load_sp500_tickers():
    url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
    df = pd.read_csv(url)

    print("üìä Loaded columns:", df.columns.tolist())  # Debugging

    # Fix column names if needed
    if 'Name' not in df.columns or 'Symbol' not in df.columns:
        if len(df.columns) >= 2:
            df.columns = ['Symbol', 'Name'] + list(df.columns[2:])
        else:
            raise ValueError("CSV does not have expected columns.")

    return {row['Name'].lower(): row['Symbol'] for _, row in df.iterrows()}

# ‚úÖ Named Entity Recognition + Ticker Extraction
def extract_companies_from_articles(news_df, known_companies):
    """
    Extracts company mentions from a DataFrame of news articles and maps them to S&P 500 tickers.

    Args:
        news_df (DataFrame): News articles with 'title' and 'description' columns
        known_companies (dict): Mapping of company names (lowercase) to tickers

    Returns:
        List of matched tickers
    """
    mentioned_tickers = set()
    articles = news_df.to_dict(orient="records")  # ‚úÖ Ensure correct format

    for article in articles:
        text = (article.get("title") or "") + " " + (article.get("description") or "")
        doc = nlp(text)

        for ent in doc.ents:
            if ent.label_ == "ORG":
                company_name = ent.text.lower()
                for known_name, ticker in known_companies.items():
                    if company_name in known_name:  # simple fuzzy match
                        mentioned_tickers.add(ticker)

    return list(mentioned_tickers)

news_df = fetch_yesterdays_news()
known_companies = load_sp500_tickers()

mentioned_tickers = extract_companies_from_articles(news_df, known_companies)
print("üß† Tickers mentioned in yesterday‚Äôs news:", mentioned_tickers)

"""## ***Step 3*** ‚Äî Sentiment Analysis with FinBERT
Analyze sentiment of the headlines (from NewsAPI) and any historical headlines you have using FinBERT, a financial-domain BERT model.
"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
from collections import defaultdict

# ‚úÖ Load FinBERT
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")

# ‚úÖ Get sentiment for a piece of text
def get_finbert_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()[0]
    sentiment_idx = np.argmax(probs)
    sentiment_label = ["negative", "neutral", "positive"][sentiment_idx]
    score = probs[sentiment_idx]
    return sentiment_label, float(score)

# ‚úÖ Score sentiment ONLY for tickers from Step 2 (your extracted tickers)
def score_sentiment_for_mentioned_tickers(news_df, known_companies, mentioned_tickers):
    # Reverse mapping: Ticker -> Company name (e.g. "MSFT": "Microsoft Corporation")
    ticker_to_name = {v: k for k, v in known_companies.items() if v in mentioned_tickers}

    sentiment_records = defaultdict(list)
    articles = news_df.to_dict(orient="records")

    for article in articles:
        text = (article.get("title") or "") + " " + (article.get("description") or "")
        text_lower = text.lower()
        sentiment, score = get_finbert_sentiment(text)

        for ticker in mentioned_tickers:
            company_name = ticker_to_name.get(ticker, "").lower()
            if ticker.lower() in text_lower or company_name in text_lower:
                sentiment_records[ticker].append((sentiment, score))

    # Aggregate
    result = []
    for ticker, entries in sentiment_records.items():
        scores = [s for _, s in entries]
        sentiments = [label for label, _ in entries]
        avg_score = np.mean(scores)
        dominant = max(set(sentiments), key=sentiments.count)
        result.append({
            "Ticker": ticker,
            "Mentions": len(entries),
            "Avg Sentiment Score": round(avg_score, 3),
            "Dominant Sentiment": dominant
        })

    return pd.DataFrame(result)

sentiment_df = score_sentiment_for_mentioned_tickers(news_df, known_companies, mentioned_tickers)
print(sentiment_df)

"""##***Step 4*** ‚Äî Fetch historical price data for these tickers using yfinance

Fetch daily stock price data for the tickers extracted from yesterday‚Äôs news (sentiment_df) using yfinance, covering the past 2 years.
"""

import yfinance as yf
import pandas as pd

# ‚úÖ STEP 4 - GOAL:
# Retrieve daily closing price data (last 2 years) for all tickers analyzed in sentiment_df

# üìå Dynamically extract tickers from sentiment_df
tickers_to_fetch = sentiment_df["Ticker"].dropna().unique().tolist()

# üéØ Fetch function
def fetch_price_history(tickers, years=5):
    end = pd.Timestamp.today()
    start = end - pd.DateOffset(years=years)
    historical_data = {}

    for ticker in tickers:
        try:
            df = yf.download(ticker, start=start.strftime('%Y-%m-%d'), end=end.strftime('%Y-%m-%d'), progress=False)
            df.reset_index(inplace=True)
            df = df[["Date", "Close"]].rename(columns={"Close": "Price"})
            historical_data[ticker] = df
        except Exception as e:
            print(f"‚ùå Failed to fetch {ticker}: {e}")

    return historical_data

# ‚úÖ Fetch and preview
historical_price_data = fetch_price_history(tickers_to_fetch)

# üëÄ Preview top 5 rows for the first 3 tickers
for ticker in list(historical_price_data.keys())[:3]:
    print(f"\nüìà {ticker} - Sample Data:\n", historical_price_data[ticker].head(50))

df = historical_price_data['FANG']
print("Earliest Date:", df['Date'].min())
print("Latest Date:", df['Date'].max())
print("Rows:", len(df))

"""## ***Step 5*** - Forecast Next 1 Hour (12 Bars Ahead) with TimeGPT"""

import requests
import json

def forecast_with_timegpt(series, horizon=12):
    url = "https://api.nixtla.io/forecast"
    headers = {
        "Authorization": "Bearer nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS",
        "Content-Type": "application/json"
    }

    payload = {
        "series": series,
        "horizon": horizon,
        "freq": "D",
        "target": "value",
        "include_history": False
    }

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    result = response.json()

    return pd.DataFrame({
        "timestamp": result["timestamp"],
        "forecast": result["value"]
    })

# Step 1: Get 5-min data
series_5min = fetch_intraday_data_yfinance("ED", period_days=30)

series_daily = resample_to_daily(series_5min)
forecast_df = forecast_with_timegpt(series_daily, horizon=7)  # next 7 days

# Show forecast
print(forecast_df)

sample_series = [
    {"timestamp": "2023-01-01T00:00:00Z", "value": 100},
    {"timestamp": "2023-01-02T00:00:00Z", "value": 101},
    {"timestamp": "2023-01-03T00:00:00Z", "value": 102},
    {"timestamp": "2023-01-04T00:00:00Z", "value": 103},
    {"timestamp": "2023-01-05T00:00:00Z", "value": 104},
    {"timestamp": "2023-01-06T00:00:00Z", "value": 105},
]

forecast = forecast_with_timegpt(sample_series, horizon=3)
print(forecast)

for ticker in sentiment_df["Ticker"]:
    series = fetch_intraday_data_yfinance(ticker, period_days=30)
    print(f"Ticker: {ticker}, Data points: {len(series)}")
    print("First 2 rows:", series[:2])
    forecast = forecast_with_timegpt(series, horizon=12)
    print(f"\nüìà {ticker} Forecast:", forecast)

"""nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS"""

import requests
import json
import pandas as pd

def forecast_with_timegpt(series, horizon=12):
    url = "https://api.nixtla.io/forecast"
    headers = {
        "Authorization": "Bearer nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS",  # Ensure this API key is valid and not in demo mode
        "Content-Type": "application/json"
    }

    payload = {
        "series": series,
        "horizon": horizon,
        "freq": "D",           # Daily frequency; change if needed
        "target": "value",
        "include_history": False
    }

    # Debug print: check the payload being sent.
    print("DEBUG: Sending payload:", json.dumps(payload, indent=2))

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    result = response.json()

    # Debug print: check the result returned by the API.
    print("DEBUG: Received result:", json.dumps(result, indent=2))

    return pd.DataFrame({
        "timestamp": result.get("timestamp", []),
        "forecast": result.get("value", [])
    })

# Code to fetch 5-min data and resample to daily
import yfinance as yf

def fetch_intraday_data_yfinance(ticker, period_days=30):
    df = yf.download(ticker, period=f"{period_days}d", interval="5m", progress=False)
    # Flatten multi-index columns (if present)
    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
    df = df[['Close']].reset_index()
    # Rename the columns if necessary (adjust if the column name is different)
    df.rename(columns={'Datetime': 'timestamp', 'Close': 'value'}, inplace=True)
    # Convert timestamp to ISO format
    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    return df.to_dict(orient='records')

def resample_to_daily(series_5min):
    df = pd.DataFrame(series_5min)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    daily_df = df['value'].resample('1D').mean().dropna().reset_index()
    daily_df['timestamp'] = daily_df['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    return daily_df.to_dict(orient='records')

# Fetch and process data
series_5min = fetch_intraday_data_yfinance("ED", period_days=30)
print("First 5 rows of 5-min data:", series_5min[:5])

series_daily = resample_to_daily(series_5min)
print("Daily resampled data:", series_daily[:5])

forecast_df = forecast_with_timegpt(series_daily, horizon=7)  # forecast next 7 days
print("Forecast DataFrame:\n", forecast_df)

# Example using a sample series
sample_series = [
    {"timestamp": "2023-01-01T00:00:00Z", "value": 100},
    {"timestamp": "2023-01-02T00:00:00Z", "value": 101},
    {"timestamp": "2023-01-03T00:00:00Z", "value": 102},
    {"timestamp": "2023-01-04T00:00:00Z", "value": 103},
    {"timestamp": "2023-01-05T00:00:00Z", "value": 104},
    {"timestamp": "2023-01-06T00:00:00Z", "value": 105},
]

forecast_sample = forecast_with_timegpt(sample_series, horizon=3)
print("Forecast for sample_series:\n", forecast_sample)

# Process forecasts for sentiment tickers (example loop)
for ticker in sentiment_df["Ticker"]:
    series = fetch_intraday_data_yfinance(ticker, period_days=30)
    print(f"Ticker: {ticker}, Data points: {len(series)}")
    print("First 2 rows:", series[:2])
    forecast = forecast_with_timegpt(series, horizon=12)
    print(f"\nüìà {ticker} Forecast:", forecast)