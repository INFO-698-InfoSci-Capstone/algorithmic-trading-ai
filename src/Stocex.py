# -*- coding: utf-8 -*-
"""Stocex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1il9tnnU4GzqnlHwKxhO0pj8IY5IqzPEs

## **Stocex project ipynb file.**
"""

# Necessary libraries
#!pip install yfinance
#!pip install newspaper3k
#!pip install transformers torch
#!pip install yfinance
#!pip install chronos-ts --upgrade --quiet
import json

"""## ***Step 1*** - Retrieve latest news from NewsAPI

Pull yesterday‚Äôs financial news headlines using NewsAPI for the latest market-moving events
"""

# Code to retrieve yesterday news from NewSAPI.

import requests
import pandas as pd
from datetime import datetime, timedelta

# üîë Enter your NewsAPI key here
NEWSAPI_KEY = "c32779e494d04276b24ac0eb577c5ca2"

def fetch_yesterdays_news():
    yesterday = datetime.now() - timedelta(days=1)
    date_str = yesterday.strftime("%Y-%m-%d")

    query = (
    "stocks OR stock OR market OR earnings OR inflation OR layoffs OR fed OR economic data "
    "OR acquisition OR merger OR buyout OR billion OR million OR IPO OR funding "
    "OR forecast OR guidance OR quarterly results OR revenue OR profits OR shares "
    "OR dividends OR buybacks OR takeover OR analysts OR downgrade OR upgrade"
    )

    domains = (
        "bloomberg.com,cnn.com,cnbc.com,wsj.com,reuters.com,marketwatch.com,"
        "yahoo.com,investopedia.com,seekingalpha.com,fool.com,fortune.com,"
        "forbes.com,techcrunch.com,businessinsider.com,barrons.com"
    )

    url = (
        f"https://newsapi.org/v2/everything?q={query}"
        f"&from={date_str}&to={date_str}"
        f"&language=en&sortBy=publishedAt"
        f"&pageSize=100"
        f"&domains={domains}"
        f"&apiKey={NEWSAPI_KEY}"
    )

    response = requests.get(url)
    data = response.json()

    if "articles" in data:
        articles = data["articles"]
        df = pd.DataFrame([{
            "title": article["title"],
            "description": article["description"],
            "publishedAt": article["publishedAt"],
            "source": article["source"]["name"]
        } for article in articles])
        return df
    else:
        print("No articles found or error in API call.")
        return pd.DataFrame()

# üöÄ Run it
news_df = fetch_yesterdays_news()
news_df.to_csv("news_headlines.csv", index=False)
news_df.head(10)  # Preview the headlines

"""##***Step 2*** - Extract Tickers from Yesterday‚Äôs News

Identify all the stock tickers mentioned in yesterday‚Äôs headlines using NLP
"""

import spacy
nlp = spacy.load("en_core_web_sm")

#Loading company names and tickers
import pandas as pd

def load_sp500_tickers():
    url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
    df = pd.read_csv(url)

    print("üìä Loaded columns:", df.columns.tolist())  # Debugging

    # Fix column names if needed
    if 'Name' not in df.columns or 'Symbol' not in df.columns:
        if len(df.columns) >= 2:
            df.columns = ['Symbol', 'Name'] + list(df.columns[2:])
        else:
            raise ValueError("CSV does not have expected columns.")

    return {row['Name'].lower(): row['Symbol'] for _, row in df.iterrows()}

# ‚úÖ Named Entity Recognition + Ticker Extraction
def extract_companies_from_articles(news_df, known_companies):
    """
    Extracts company mentions from a DataFrame of news articles and maps them to S&P 500 tickers.

    Args:
        news_df (DataFrame): News articles with 'title' and 'description' columns
        known_companies (dict): Mapping of company names (lowercase) to tickers

    Returns:
        List of matched tickers
    """
    mentioned_tickers = set()
    articles = news_df.to_dict(orient="records")  # ‚úÖ Ensure correct format

    for article in articles:
        text = (article.get("title") or "") + " " + (article.get("description") or "")
        doc = nlp(text)

        for ent in doc.ents:
            if ent.label_ == "ORG":
                company_name = ent.text.lower()
                for known_name, ticker in known_companies.items():
                    if company_name in known_name:  # simple fuzzy match
                        mentioned_tickers.add(ticker)

    return list(mentioned_tickers)

news_df = fetch_yesterdays_news()
known_companies = load_sp500_tickers()

mentioned_tickers = extract_companies_from_articles(news_df, known_companies)
print("üß† Tickers mentioned in yesterday‚Äôs news:", mentioned_tickers)

"""## ***Step 3*** ‚Äî Sentiment Analysis with FinBERT
Analyze sentiment of the headlines (from NewsAPI) and any historical headlines you have using FinBERT, a financial-domain BERT model.
"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
from collections import defaultdict

# ‚úÖ Load FinBERT
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")

# ‚úÖ Get sentiment for a piece of text
def get_finbert_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()[0]
    sentiment_idx = np.argmax(probs)
    sentiment_label = ["negative", "neutral", "positive"][sentiment_idx]
    score = probs[sentiment_idx]
    return sentiment_label, float(score)

# ‚úÖ Score sentiment ONLY for tickers from Step 2 (your extracted tickers)
def score_sentiment_for_mentioned_tickers(news_df, known_companies, mentioned_tickers):
    from collections import defaultdict
    import numpy as np
    import pandas as pd

    # üîÑ Reverse map tickers -> company names
    ticker_to_name = {
        ticker: name
        for name, ticker in known_companies.items()
        if ticker in mentioned_tickers
    }

    sentiment_records = defaultdict(list)

    for _, article in news_df.iterrows():
        text = f"{article.get('title', '')} {article.get('description', '')}".lower()
        sentiment, score = get_finbert_sentiment(text)

        for ticker in mentioned_tickers:
            company_name = ticker_to_name.get(ticker, "").lower()
            if ticker.lower() in text or company_name in text:
                sentiment_records[ticker].append((sentiment, score))

    # üìä Aggregate results
    results = []
    for ticker, records in sentiment_records.items():
        sentiments = [s for s, _ in records]
        scores = [s for _, s in records]
        avg_score = np.mean(scores)

        # ü™µ Debug print (optional)
        print(f"üîç {ticker} ‚Üí Avg Sentiment Score: {avg_score:.3f}")

        if avg_score >= 0.98:  # ‚úÖ Only keep perfect scores
            dominant = max(set(sentiments), key=sentiments.count)
            results.append({
                "Ticker": ticker,
                "Mentions": len(records),
                "Avg Sentiment Score": round(avg_score, 3),
                "Dominant Sentiment": dominant
            })

    # üõ°Ô∏è Handle empty result
    df = pd.DataFrame(results)
    if not df.empty:
        df = df.sort_values("Mentions", ascending=False).reset_index(drop=True)
    return df

sentiment_df = score_sentiment_for_mentioned_tickers(news_df, known_companies, mentioned_tickers)
sentiment_df.to_csv("sentiment_summary.csv", index=False)
print(sentiment_df)

"""##***Step 4*** ‚Äî Fetch historical price data for these tickers using yfinance"""

# === Intraday Fetch & Resample Functions ===
def fetch_intraday_data_yfinance(ticker, period_days=30, interval="5m"):
    data = yf.download(tickers=ticker, period=f"{period_days}d", interval=interval, progress=False)
    data = data.reset_index()
    data.rename(columns={"Datetime": "timestamp", "Close": "value"}, inplace=True)
    return data[["timestamp", "value"]]

def resample_to_daily(df):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    daily_df = df.resample('1D').last().dropna().reset_index()
    return daily_df

"""Fetch daily stock price data for the tickers extracted from yesterday‚Äôs news (sentiment_df) using yfinance, covering the past 5 years.


"""

import yfinance as yf
import pandas as pd

# ‚úÖ STEP 4 - GOAL:
# Retrieve daily closing price data (last 2 years) for all tickers analyzed in sentiment_df

# üìå Dynamically extract tickers from sentiment_df
tickers_to_fetch = sentiment_df["Ticker"].dropna().unique().tolist()

# üéØ Fetch function
def fetch_price_history(tickers, years=5):
    end = pd.Timestamp.today()
    start = end - pd.DateOffset(years=years)
    historical_data = {}

    for ticker in tickers:
        try:
            df = yf.download(ticker, start=start.strftime('%Y-%m-%d'), end=end.strftime('%Y-%m-%d'), progress=False)
            df.reset_index(inplace=True)
            df = df[["Date", "Close"]].rename(columns={"Close": "Price"})
            historical_data[ticker] = df
        except Exception as e:
            print(f"‚ùå Failed to fetch {ticker}: {e}")

    return historical_data

# ‚úÖ Fetch and preview
historical_price_data = fetch_price_history(tickers_to_fetch)

# üëÄ Preview top 5 rows for the first 3 tickers
for ticker in list(historical_price_data.keys())[:3]:
    print(f"\nüìà {ticker} - Sample Data:\n", historical_price_data[ticker].head(50))

import os

# Create a directory to store CSVs for each ticker
output_dir = "historical_price_data"
os.makedirs(output_dir, exist_ok=True)

# Save each ticker's historical price data
for ticker, df in historical_price_data.items():
    filename = f"{output_dir}/{ticker}_price_history.csv"
    df.to_csv(filename, index=False)

print(historical_price_data.keys())



"""## ***Step 5*** - Forecast Next 1 Hour (12 Bars Ahead) with TimeGPT"""

import requests
import json

def forecast_with_timegpt(series, horizon=7):
    url = "https://api.nixtla.io/forecast"
    headers = {
        "Authorization": "Bearer nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS",
        "Content-Type": "application/json"
    }

    # ‚úÖ Convert DataFrame to JSON serializable format
    payload = {
        "series": series.to_dict(orient="records"),
        "horizon": horizon,
        "freq": "D",
        "target": "value",
        "include_history": False
    }

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    result = response.json()

    return pd.DataFrame({
        "timestamp": result["timestamp"],
        "forecast": result["value"]
    })

# Step 1: Get 5-min data
series_5min = fetch_intraday_data_yfinance("ED", period_days=30)

# Step 2: Resample and clean
series_daily = resample_to_daily(series_5min)
series_daily.columns = ["timestamp", "value"]  # Overwrite any weird tuple columns
series_daily["timestamp"] = series_daily["timestamp"].astype(str)

# Step 3: Forecast
forecast_df = forecast_with_timegpt(series_daily, horizon=7)

# Step 4: Show result
print(forecast_df)



# ‚úÖ Step 5: Run forecast on sample static data (manual test)
sample_series = [
    {"timestamp": "2023-01-01T00:00:00Z", "value": 100},
    {"timestamp": "2023-01-02T00:00:00Z", "value": 101},
    {"timestamp": "2023-01-03T00:00:00Z", "value": 102},
    {"timestamp": "2023-01-04T00:00:00Z", "value": 103},
    {"timestamp": "2023-01-05T00:00:00Z", "value": 104},
    {"timestamp": "2023-01-06T00:00:00Z", "value": 105},
]

# Convert to DataFrame
sample_series_df = pd.DataFrame(sample_series)

# Forecast
forecast = forecast_with_timegpt(sample_series_df, horizon=3)
print(forecast)

for ticker in sentiment_df["Ticker"].dropna().unique().tolist():
    try:
        # Step 1: Fetch intraday
        series = fetch_intraday_data_yfinance(ticker, period_days=30)

        # ‚úÖ Step 2: Clean DataFrame
        series = series.reset_index(drop=True)            # Remove multi-index if present
        series.columns = ["timestamp", "value"]           # Flatten any weird column names
        series["timestamp"] = pd.to_datetime(series["timestamp"]).astype(str)  # Convert datetime to string

        print(f"\nüîé Ticker: {ticker}, Data points: {len(series)}")
        print("üìä First 2 rows:\n", series.head(2))

        # Step 3: Forecast
        forecast = forecast_with_timegpt(series, horizon=12)

        print(f"\nüìà {ticker} Forecast:\n", forecast)

    except Exception as e:
        print(f"‚ùå Error forecasting {ticker}: {e}")

"""nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS"""

import requests
import json
import pandas as pd

def forecast_with_timegpt(series, horizon=12):
    url = "https://api.nixtla.io/forecast"
    headers = {
        "Authorization": "Bearer nixak-oxZzNGjeG0nrFYWsOcWaehcyysm74OCuNeL2s6Y0wTX7df98QBnDEdTSce2DyO3rLL3C3bvHakkkokIS",  # Ensure this API key is valid and not in demo mode
        "Content-Type": "application/json"
    }

    payload = {
        "series": series,
        "horizon": horizon,
        "freq": "D",           # Daily frequency; change if needed
        "target": "value",
        "include_history": False
    }

    # Debug print: check the payload being sent.
    print("DEBUG: Sending payload:", json.dumps(payload, indent=2))

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    result = response.json()

    # Debug print: check the result returned by the API.
    print("DEBUG: Received result:", json.dumps(result, indent=2))

    return pd.DataFrame({
        "timestamp": result.get("timestamp", []),
        "forecast": result.get("value", [])
    })

# Code to fetch 5-min data and resample to daily
import yfinance as yf

def fetch_intraday_data_yfinance(ticker, period_days=30):
    df = yf.download(ticker, period=f"{period_days}d", interval="5m", progress=False)
    # Flatten multi-index columns (if present)
    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
    df = df[['Close']].reset_index()
    # Rename the columns if necessary (adjust if the column name is different)
    df.rename(columns={'Datetime': 'timestamp', 'Close': 'value'}, inplace=True)
    # Convert timestamp to ISO format
    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    return df.to_dict(orient='records')

def resample_to_daily(series_5min):
    df = pd.DataFrame(series_5min)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    daily_df = df['value'].resample('1D').mean().dropna().reset_index()
    daily_df['timestamp'] = daily_df['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    return daily_df.to_dict(orient='records')

# Fetch and process data
series_5min = fetch_intraday_data_yfinance("ED", period_days=30)
print("First 5 rows of 5-min data:", series_5min[:5])

series_daily = resample_to_daily(series_5min)
print("Daily resampled data:", series_daily[:5])

forecast_df = forecast_with_timegpt(series_daily, horizon=7)  # forecast next 7 days
print("Forecast DataFrame:\n", forecast_df)

# Example using a sample series
sample_series = [
    {"timestamp": "2023-01-01T00:00:00Z", "value": 100},
    {"timestamp": "2023-01-02T00:00:00Z", "value": 101},
    {"timestamp": "2023-01-03T00:00:00Z", "value": 102},
    {"timestamp": "2023-01-04T00:00:00Z", "value": 103},
    {"timestamp": "2023-01-05T00:00:00Z", "value": 104},
    {"timestamp": "2023-01-06T00:00:00Z", "value": 105},
]

forecast_sample = forecast_with_timegpt(sample_series, horizon=3)
print("Forecast for sample_series:\n", forecast_sample)

# Process forecasts for sentiment tickers (example loop)
for ticker in sentiment_df["Ticker"]:
    series = fetch_intraday_data_yfinance(ticker, period_days=30)
    print(f"Ticker: {ticker}, Data points: {len(series)}")
    print("First 2 rows:", series[:2])
    forecast = forecast_with_timegpt(series, horizon=12)
    print(f"\nüìà {ticker} Forecast:", forecast)

"""Step:6. Superposed Epoch Analysis (SEA)"""

import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
from datetime import timedelta

# STEP 1: Filter high-sentiment events for one ticker (e.g., EG)
def get_high_sentiment_events(ticker, threshold=0.7):
    df = sentiment_df[sentiment_df["Ticker"] == ticker].copy()
    df["Date"] = pd.to_datetime(df["Date"])
    return df[df["Avg Sentiment Score"] >= threshold]["Date"].tolist()

# STEP 2: Fetch daily stock prices
def get_daily_price_data(ticker, start_date, end_date):
    data = yf.download(ticker, start=start_date, end=end_date, interval="1d", progress=False)
    data = data[["Close"]].reset_index()
    data.columns = ["Date", "Price"]
    return data

# STEP 3: Extract price windows around each event
def extract_price_windows(price_df, event_dates, window=(-3, 5)):
    matrix = []
    price_df.set_index("Date", inplace=True)

    for event_date in event_dates:
        try:
            dates = [event_date + timedelta(days=offset) for offset in range(window[0], window[1] + 1)]
            prices = price_df.reindex(dates)["Price"].values

            if not pd.isnull(prices).any():
                matrix.append(prices)
        except Exception:
            continue

    return pd.DataFrame(matrix)

# STEP 4: Plot SEA result
def plot_superposed_epoch(matrix, window=(-3, 5)):
    mean_pattern = matrix.mean()
    rel_days = list(range(window[0], window[1] + 1))

    plt.figure(figsize=(10, 5))
    plt.plot(rel_days, mean_pattern, marker="o", label="Average Price Movement")
    plt.axvline(0, color="red", linestyle="--", label="Event Day")
    plt.xlabel("Days from Event")
    plt.ylabel("Price")
    plt.title("Superposed Epoch Analysis (Sentiment-Driven)")
    plt.legend()
    plt.grid(True)
    plt.show()

# Add fake dates to your current sentiment_df
sentiment_df["Date"] = [datetime.today().date() - timedelta(days=i) for i in range(len(sentiment_df))]
sentiment_df["Date"] = pd.to_datetime(sentiment_df["Date"])

# Run SEA for ticker EG
target_ticker = "AMZN"  # or "AAPL" if you like
event_dates = [pd.to_datetime("2024-03-01"), pd.to_datetime("2024-03-15"), pd.to_datetime("2024-03-29")]  # sample dates

start = min(event_dates) - timedelta(days=5)
end = max(event_dates) + timedelta(days=5)

price_data = get_daily_price_data(target_ticker, start, end)
event_matrix = extract_price_windows(price_data, event_dates)
plot_superposed_epoch(event_matrix)

print("Event Dates Found:", event_dates)
print("Shape of event_matrix:", event_matrix.shape)
print("event_matrix preview:\n", event_matrix.head())